{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import re\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classify20:\n",
    "    def myTokenizer(self,text):\n",
    "        return nltk.regexp_tokenize(text, \"\\\\b[a-zA-Z]{3,}\\\\b\")\n",
    "    def tokenizeContents(self,contents):\n",
    "        li=re.findall(self.regex,contents)\n",
    "        return [self.myTokenizer(content) for content in li]\n",
    "    def preprocess(self):\n",
    "        self.model = gensim.models.KeyedVectors.load_word2vec_format('C:/Users/Arfat/Downloads/Compressed/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "        self.twenty_all = fetch_20newsgroups(subset='all', shuffle=True,remove=('headers', 'footers', 'quotes'))\n",
    "        full1=[\" \".join(data.split(\"\\n\")) for data in self.twenty_all.data]\n",
    "        stopWords = set(stopwords.words('english'))\n",
    "        self.full= []\n",
    "        tmp=[] \n",
    "        for d in range(len(full1)):\n",
    "            words = word_tokenize(full1[d])\n",
    "            for w in words:\n",
    "                if w not in stopWords:\n",
    "                    tmp.append(w)\n",
    "            x=\" \".join(tmp)\n",
    "            tmp=self.myTokenizer(x)\n",
    "            x=\" \".join(tmp)\n",
    "            tmp=[]\n",
    "            self.full.append(x)\n",
    "        self.skf = StratifiedKFold(n_splits=3,shuffle=True,random_state=1)\n",
    "        y=self.twenty_all.target\n",
    "        self.regex = r'\\b\\w+\\b'\n",
    "        self.readydocs()\n",
    "    def readydocs(self):\n",
    "        add=np.zeros(300,dtype='float32')\n",
    "        self.doc=np.zeros((18846,300))\n",
    "        self.l=[]#for empty documents\n",
    "        count=0 #total times the key error is raised\n",
    "        c_names=[] #contains words that are in documents but they don't have vector representations\n",
    "        count_word_exists=0 #checks number of words having vector representations in the current document\n",
    "        self.del_index=[] # has index of the documnets where no word has a vector representation\n",
    "\n",
    "        rate=[] #hit rate for words having vector representations in a document\n",
    "        for q1 in range(len(self.full)):\n",
    "            token=self.tokenizeContents(self.full[q1])\n",
    "            token = list(filter(None, token))\n",
    "            if len(token)==0:\n",
    "                self.l.append(q1)\n",
    "                rate.append(0)\n",
    "                self.doc[q1]=np.zeros(300,dtype='float32')\n",
    "                continue\n",
    "            for q in range(len(token)):\n",
    "                try:\n",
    "                    add=add+self.model.word_vec(\"\".join(token[q]))\n",
    "                    count_word_exists=count_word_exists+1\n",
    "\n",
    "                except KeyError:\n",
    "                    count+=1\n",
    "                    if \"\".join(token[q]) not in c_names:\n",
    "                        c_names.append(\"\".join(token[q]))\n",
    "                    continue\n",
    "        #print(add,len(token))\n",
    "            if count_word_exists==0:\n",
    "                self.del_index.append(q1)\n",
    "                add=np.zeros(300,dtype='float32')\n",
    "                self.doc[q1]=add\n",
    "                rate.append(0)\n",
    "                continue\n",
    "            rate.append(count_word_exists/len(token))\n",
    "            add=add/count_word_exists\n",
    "            self.doc[q1]=add\n",
    "            add=np.zeros(300,dtype='float32')\n",
    "            count_word_exists=0\n",
    "        self.doc=[ value for (i, value) in enumerate(self.doc) if i not in set(self.l+self.del_index) ]\n",
    "        self.doc=np.asarray(self.doc)\n",
    "        self.delundocs()###delete unnecessary docs\n",
    "    def delundocs(self):\n",
    "        self.y=self.twenty_all.target                                 ##############deleting docs for all methods\n",
    "        self.full=[ value for (i, value) in enumerate(self.full) if i not in set(self.l+self.del_index) ]\n",
    "        for i in sorted(set(self.l+self.del_index),reverse=True):\n",
    "            self.y=np.delete(self.y,i)\n",
    "    def w2v(self):\n",
    "    ##########W2V\n",
    "        clf=LinearSVC()\n",
    "        for k,(train_index, test_index) in enumerate(self.skf.split(self.doc, self.y)):\n",
    "            clf.fit(self.doc[train_index],self.y[train_index])\n",
    "            y_pred=clf.predict(self.doc[test_index])\n",
    "            print(\"Fold :\",k)\n",
    "            print(classification_report(y_pred,self.y[test_index],target_names=self.twenty_all.target_names))\n",
    "    def tfidf(self):\n",
    "    ##################              TF-IDF\n",
    "        tfvec=TfidfVectorizer(ngram_range=(1,2))\n",
    "        X=tfvec.fit_transform(self.full)\n",
    "        clf = MultinomialNB()\n",
    "        for k,(train_index, test_index) in enumerate(self.skf.split(X, self.y)):\n",
    "\n",
    "            clf.fit(X[train_index],self.y[train_index])\n",
    "            y_pred=clf.predict(X[test_index])\n",
    "            print(\"Fold :\",k)\n",
    "            print(classification_report(y_pred,self.y[test_index],target_names=self.twenty_all.target_names))\n",
    "    def ftext(self,cmd):\n",
    "    ##################              Fast text\n",
    "        cmd1=\"C:/Users/Arfat/Downloads/Compressed/amazonreviews/fasttext supervised \"\n",
    "        cmd1=cmd1+cmd\n",
    "        cmd1=cmd1+\" -input train20.txt -output model20 >> ftrainresult.txt\"\n",
    "        #print (cmd1)\n",
    "        #return\n",
    "        cmd2=\"C:/Users/Arfat/Downloads/Compressed/amazonreviews/fasttext test model20.bin test20.txt >> ftestresult.txt\"\n",
    "        cmd3=\"C:/Users/Arfat/Downloads/Compressed/amazonreviews/fasttext predict model20.bin test20.txt > result20.txt\"\n",
    "        c=0\n",
    "        for k,(train_index, test_index) in enumerate(self.skf.split(self.full, self.y)):\n",
    "            outF = open(\"train20.txt\", \"w\")\n",
    "            out1 = open(\"test20.txt\", \"w\")\n",
    "            for i in range(len(train_index)):\n",
    "                outF.write(\"__label__\")\n",
    "                outF.write(str(self.y[train_index[i]]))\n",
    "                outF.write(\" \")\n",
    "                try :\n",
    "                    outF.write(self.full[train_index[i]])\n",
    "                    outF.write(\"\\n\")\n",
    "                except UnicodeEncodeError:\n",
    "                    print(\"i\",i)\n",
    "                    outF.write(\"\\n\")\n",
    "                    continue\n",
    "            os.system(cmd1)\n",
    "            print(len(test_index))\n",
    "            c=0\n",
    "            for j in range(len(test_index)):\n",
    "                out1.write(\"__label__\")\n",
    "                out1.write(str(self.y[test_index[j]]))\n",
    "                out1.write(\" \")\n",
    "                c=c+1\n",
    "                try :\n",
    "                    out1.write(self.full[test_index[j]])\n",
    "                    out1.write(\"\\n\")\n",
    "                except UnicodeEncodeError:\n",
    "                    print(\"j\",j)\n",
    "                    out1.write(\"\\n\")\n",
    "                    continue\n",
    "            outF.close()\n",
    "            out1.close()\n",
    "            os.system(cmd2)\n",
    "            os.system(cmd3)\n",
    "            f = open('result20.txt', 'r')\n",
    "            lines = np.asarray([np.int32(\"\".join(re.findall('\\d+', line))) for line in f.readlines()])\n",
    "            f.close()\n",
    "            print(classification_report(lines,self.y[test_index],target_names=self.twenty_all.target_names))\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c20=Classify20()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c20.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c20.tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c20.w2v()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c20.ftext('-dim 100 -epoch 100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
