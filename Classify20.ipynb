{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arfat\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import re\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report"
    ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classify20:\n",
    "    def avgfolds(self,txt):\n",
    "        x1=pd.read_csv(txt+\"0.csv\")\n",
    "        x2=pd.read_csv(txt+\"1.csv\")\n",
    "        x3=pd.read_csv(txt+\"2.csv\")\n",
    "        X=pd.concat((x1,x2,x3))\n",
    "        by_row_index = X.groupby(X.index)\n",
    "        df_means = by_row_index.mean()\n",
    "        n=df_means.round(2)\n",
    "        n.insert(loc=0,column='Class',value=self.twenty_all.target_names)\n",
    "        n=n.drop('Support',axis=1)\n",
    "        n.loc['20']=n.mean()\n",
    "        n.loc['20','Class']='avg'\n",
    "        n=n.round(2)\n",
    "        n.to_csv(txt+'consolidate.csv',index = False)\n",
    "\n",
    "    def reporttocsv(self,txt,i):\n",
    "        report_data = []\n",
    "        lines = self.report.split('\\n')\n",
    "        x=0\n",
    "        for line in lines[2:-3]:\n",
    "            row = {}\n",
    "            d=re.findall('(\\d+(\\.\\d{1,2})?|\\d+)', line)\n",
    "            row['Class'] = self.twenty_all.target_names[x]\n",
    "            row['Precision'] = d[0][0]\n",
    "            row['Recall'] = d[1][0]\n",
    "            row['F1_score'] = d[2][0]\n",
    "            row['Support'] = d[3][0]\n",
    "            report_data.append(row)\n",
    "            x=x+1\n",
    "        dataframe = pd.DataFrame.from_dict(report_data)\n",
    "        dataframe.to_csv(txt+str(i)+'.csv', index = False)\n",
    "    def myTokenizer(self,text):\n",
    "        return nltk.regexp_tokenize(text, \"\\\\b[a-zA-Z]{3,}\\\\b\")\n",
    "    def tokenizeContents(self,contents):\n",
    "        li=re.findall(self.regex,contents)\n",
    "        return [self.myTokenizer(content) for content in li]\n",
    "    def preprocess(self):\n",
    "        self.model = gensim.models.KeyedVectors.load_word2vec_format('C:/Users/Arfat/Downloads/Compressed/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "        self.twenty_all = fetch_20newsgroups(subset='all', shuffle=True,remove=('headers', 'footers', 'quotes'))\n",
    "        full1=[\" \".join(data.split(\"\\n\")) for data in self.twenty_all.data]\n",
    "        stopWords = set(stopwords.words('english'))\n",
    "        self.full= []\n",
    "        tmp=[] \n",
    "        for d in range(len(full1)):\n",
    "            words = word_tokenize(full1[d])\n",
    "            for w in words:\n",
    "                if w not in stopWords:\n",
    "                    tmp.append(w)\n",
    "            x=\" \".join(tmp)\n",
    "            tmp=self.myTokenizer(x)\n",
    "            x=\" \".join(tmp)\n",
    "            tmp=[]\n",
    "            self.full.append(x)\n",
    "        self.skf = StratifiedKFold(n_splits=3,shuffle=True,random_state=1)\n",
    "        y=self.twenty_all.target\n",
    "        self.regex = r'\\b\\w+\\b'\n",
    "        self.readydocs()\n",
    "    def readydocs(self):\n",
    "        add=np.zeros(300,dtype='float32')\n",
    "        self.doc=np.zeros((18846,300))\n",
    "        self.l=[]#for empty documents\n",
    "        count=0 #total times the key error is raised\n",
    "        c_names=[] #contains words that are in documents but they don't have vector representations\n",
    "        count_word_exists=0 #checks number of words having vector representations in the current document\n",
    "        self.del_index=[] # has index of the documnets where no word has a vector representation\n",
    "\n",
    "        rate=[] #hit rate for words having vector representations in a document\n",
    "        for q1 in range(len(self.full)):\n",
    "            token=self.tokenizeContents(self.full[q1])\n",
    "            token = list(filter(None, token))\n",
    "            if len(token)==0:\n",
    "                self.l.append(q1)\n",
    "                rate.append(0)\n",
    "                self.doc[q1]=np.zeros(300,dtype='float32')\n",
    "                continue\n",
    "            for q in range(len(token)):\n",
    "                try:\n",
    "                    add=add+self.model.word_vec(\"\".join(token[q]))\n",
    "                    count_word_exists=count_word_exists+1\n",
    "\n",
    "                except KeyError:\n",
    "                    count+=1\n",
    "                    if \"\".join(token[q]) not in c_names:\n",
    "                        c_names.append(\"\".join(token[q]))\n",
    "                    continue\n",
    "        #print(add,len(token))\n",
    "            if count_word_exists==0:\n",
    "                self.del_index.append(q1)\n",
    "                add=np.zeros(300,dtype='float32')\n",
    "                self.doc[q1]=add\n",
    "                rate.append(0)\n",
    "                continue\n",
    "            rate.append(count_word_exists/len(token))\n",
    "            add=add/count_word_exists\n",
    "            self.doc[q1]=add\n",
    "            add=np.zeros(300,dtype='float32')\n",
    "            count_word_exists=0\n",
    "        self.doc=[ value for (i, value) in enumerate(self.doc) if i not in set(self.l+self.del_index) ]\n",
    "        self.doc=np.asarray(self.doc)\n",
    "        self.delundocs()###delete unnecessary docs\n",
    "    def delundocs(self):\n",
    "        self.y=self.twenty_all.target                                 ##############deleting docs for all methods\n",
    "        self.full=[ value for (i, value) in enumerate(self.full) if i not in set(self.l+self.del_index) ]\n",
    "        for i in sorted(set(self.l+self.del_index),reverse=True):\n",
    "            self.y=np.delete(self.y,i)\n",
    "    def w2v(self):\n",
    "    ##########W2V\n",
    "        clf=LinearSVC()\n",
    "        for k,(train_index, test_index) in enumerate(self.skf.split(self.doc, self.y)):\n",
    "            clf.fit(self.doc[train_index],self.y[train_index])\n",
    "            y_pred=clf.predict(self.doc[test_index])\n",
    "            print(\"Fold :\",k)\n",
    "            self.report=classification_report(y_pred,self.y[test_index],target_names=self.twenty_all.target_names)\n",
    "            self.reporttocsv('w2v',i=k)\n",
    "        self.avgfolds('w2v')\n",
    "    def tfidf(self):\n",
    "    ##################              TF-IDF\n",
    "        tfvec=TfidfVectorizer(ngram_range=(1,2))\n",
    "        X=tfvec.fit_transform(self.full)\n",
    "        clf = MultinomialNB()\n",
    "        for k,(train_index, test_index) in enumerate(self.skf.split(X, self.y)):\n",
    "\n",
    "            clf.fit(X[train_index],self.y[train_index])\n",
    "            y_pred=clf.predict(X[test_index])\n",
    "            print(\"Fold :\",k)\n",
    "            self.report=classification_report(y_pred,self.y[test_index],target_names=self.twenty_all.target_names)\n",
    "            self.reporttocsv('tfidf',i=k)\n",
    "        self.avgfolds('tfidf')\n",
    "    def ftext(self,cmd):\n",
    "    ##################              Fast text\n",
    "        cmd1=\"C:/Users/Arfat/Downloads/Compressed/amazonreviews/fasttext supervised \"\n",
    "        cmd1=cmd1+cmd\n",
    "        cmd1=cmd1+\" -input train20.txt -output model20 >> ftrainresult.txt\"\n",
    "        #print (cmd1)\n",
    "        #return\n",
    "        cmd2=\"C:/Users/Arfat/Downloads/Compressed/amazonreviews/fasttext test model20.bin test20.txt >> ftestresult.txt\"\n",
    "        cmd3=\"C:/Users/Arfat/Downloads/Compressed/amazonreviews/fasttext predict model20.bin test20.txt > result20.txt\"\n",
    "        c=0\n",
    "        for k,(train_index, test_index) in enumerate(self.skf.split(self.full, self.y)):\n",
    "            outF = open(\"train20.txt\", \"w\")\n",
    "            out1 = open(\"test20.txt\", \"w\")\n",
    "            for i in range(len(train_index)):\n",
    "                outF.write(\"__label__\")\n",
    "                outF.write(str(self.y[train_index[i]]))\n",
    "                outF.write(\" \")\n",
    "                try :\n",
    "                    outF.write(self.full[train_index[i]])\n",
    "                    outF.write(\"\\n\")\n",
    "                except UnicodeEncodeError:\n",
    "                    print(\"i\",i)\n",
    "                    outF.write(\"\\n\")\n",
    "                    continue\n",
    "            os.system(cmd1)\n",
    "            print(len(test_index))\n",
    "            c=0\n",
    "            for j in range(len(test_index)):\n",
    "                out1.write(\"__label__\")\n",
    "                out1.write(str(self.y[test_index[j]]))\n",
    "                out1.write(\" \")\n",
    "                c=c+1\n",
    "                try :\n",
    "                    out1.write(self.full[test_index[j]])\n",
    "                    out1.write(\"\\n\")\n",
    "                except UnicodeEncodeError:\n",
    "                    print(\"j\",j)\n",
    "                    out1.write(\"\\n\")\n",
    "                    continue\n",
    "            outF.close()\n",
    "            out1.close()\n",
    "            os.system(cmd2)\n",
    "            os.system(cmd3)\n",
    "            f = open('result20.txt', 'r')\n",
    "            lines = np.asarray([np.int32(\"\".join(re.findall('\\d+', line))) for line in f.readlines()])\n",
    "            f.close()\n",
    "            self.report=classification_report(lines,self.y[test_index],target_names=self.twenty_all.target_names)\n",
    "            self.reporttocsv('fasttext',i=k)\n",
    "        self.avgfolds('fasttext')\n",
    "    def wtw2v(self): ####TFIDF weighted w2v\n",
    "        tfvec=TfidfVectorizer()\n",
    "        X=tfvec.fit_transform(self.full)\n",
    "        fn=tfvec.get_feature_names()\n",
    "        self.docvec=np.zeros((18298,300))\n",
    "        tx=[]\n",
    "        for doc in range(len(self.full)):\n",
    "            add=np.zeros(300,dtype='float32')\n",
    "            totalwt=0\n",
    "            feature_index = X[doc,:].nonzero()[1]\n",
    "            #print (feature_index)\n",
    "            tfidf_scores = zip(feature_index, [X[doc, x] for x in feature_index])\n",
    "            for w, s in [(fn[i], s) for (i, s) in tfidf_scores]:\n",
    "                totalwt=totalwt+round(s,2)\n",
    "                #print (w, s)\n",
    "                try: \n",
    "                    add=add+(round(s,2)*self.model.word_vec(w))\n",
    "                except KeyError:\n",
    "                    #totalwt=totalwt-round(s,2)\n",
    "                    continue\n",
    "            try:\n",
    "                self.docvec[doc]=add/totalwt\n",
    "                tx.append(totalwt)\n",
    "            except ZeroDivisionError:\n",
    "                print(doc,totalwt)\n",
    "                continue\n",
    "        #print(self.docvec)\n",
    "        #print(tx)\n",
    "        clf=LinearSVC()\n",
    "        for k,(train_index, test_index) in enumerate(self.skf.split(self.docvec, self.y)):\n",
    "            clf.fit(self.docvec[train_index],self.y[train_index])\n",
    "            y_pred=clf.predict(self.docvec[test_index])\n",
    "            print(\"Fold :\",k)\n",
    "            self.report=classification_report(y_pred,self.y[test_index],target_names=self.twenty_all.target_names)\n",
    "            self.reporttocsv('wtw2v',i=k)\n",
    "        self.avgfolds('wtw2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c20=Classify20()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c20.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 0\n",
      "Fold : 1\n",
      "Fold : 2\n"
     ]
    }
   ],
   "source": [
    "c20.tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 0\n",
      "Fold : 1\n",
      "Fold : 2\n"
     ]
    }
   ],
   "source": [
    "c20.w2v()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6105\n",
      "6101\n",
      "6092\n"
     ]
    }
   ],
   "source": [
    "c20.ftext(\"-dim 100 -epoch 100\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 0\n",
      "Fold : 1\n",
      "Fold : 2\n"
     ]
    }
   ],
   "source": [
    "c20.wtw2v()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
